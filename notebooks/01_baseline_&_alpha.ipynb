{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install Required Packages\n",
        "!pip install gymnasium[atari] -q\n",
        "!pip install gymnasium[accept-rom-license] -q\n",
        "!pip install ale-py -q\n",
        "!pip install torch torchvision -q\n",
        "!pip install opencv-python -q\n",
        "!pip install matplotlib -q\n",
        "\n",
        "print(\"All packages installed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UxPsYGs5Owc",
        "outputId": "89b8ca8f-2815-45d5-8208-9b8bdc46c8f5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: gymnasium 1.2.2 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mAll packages installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Environment and GPU\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import numpy as np\n",
        "import ale_py\n",
        "\n",
        "# Register Atari environments\n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "# Check GPU availability\n",
        "print(\"=\" * 50)\n",
        "print(\"SYSTEM CHECK\")\n",
        "print(\"=\" * 50)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "print()\n",
        "\n",
        "# Test Bank Heist environment\n",
        "print(\"=\" * 50)\n",
        "print(\"TESTING BANK HEIST ENVIRONMENT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "env = gym.make('ALE/BankHeist-v5', render_mode='rgb_array')\n",
        "state, info = env.reset()\n",
        "\n",
        "print(f\"Environment created successfully!\")\n",
        "print(f\"State shape (frame dimensions): {state.shape}\")\n",
        "print(f\"Number of possible actions: {env.action_space.n}\")\n",
        "print(f\"Actions available: {env.unwrapped.get_action_meanings()}\")\n",
        "\n",
        "# Take a test action\n",
        "action = env.action_space.sample()\n",
        "next_state, reward, terminated, truncated, info = env.step(action)\n",
        "print(f\"Test action completed - Reward: {reward}\")\n",
        "\n",
        "env.close()\n",
        "print()\n",
        "print(\"All tests passed! Ready to build the agent!\")\n",
        "print(\"=\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCKyvmkK5lrw",
        "outputId": "c5be7b5a-ca40-457c-e491-4d8e28d2ccc9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "SYSTEM CHECK\n",
            "==================================================\n",
            "Using device: cuda\n",
            "GPU Name: Tesla T4\n",
            "GPU Memory: 15.83 GB\n",
            "\n",
            "==================================================\n",
            "TESTING BANK HEIST ENVIRONMENT\n",
            "==================================================\n",
            "Environment created successfully!\n",
            "State shape (frame dimensions): (210, 160, 3)\n",
            "Number of possible actions: 18\n",
            "Actions available: ['NOOP', 'FIRE', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT', 'UPFIRE', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n",
            "Test action completed - Reward: 0.0\n",
            "\n",
            "All tests passed! Ready to build the agent!\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Deep Q-Network\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    \"\"\"\n",
        "    Deep Q-Network for Bank Heist\n",
        "    Takes 4 stacked grayscale frames (84x84) as input\n",
        "    Outputs Q-values for each of the 18 possible actions\n",
        "    \"\"\"\n",
        "    def __init__(self, action_size):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        # Convolutional layers to process game frames\n",
        "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)  # 4 frames -> 32 features\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2) # 32 -> 64 features\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1) # 64 -> 64 features\n",
        "\n",
        "        # Fully connected layers for decision making\n",
        "        self.fc1 = nn.Linear(7 * 7 * 64, 512)  # Flatten and connect\n",
        "        self.fc2 = nn.Linear(512, action_size)  # Output Q-value for each action\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Process frames through the network\"\"\"\n",
        "        x = F.relu(self.conv1(x))  # Learn features from frames\n",
        "        x = F.relu(self.conv2(x))  # Learn higher-level features\n",
        "        x = F.relu(self.conv3(x))  # Learn even higher-level features\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten for fully connected layers\n",
        "\n",
        "        x = F.relu(self.fc1(x))    # Decision making layer\n",
        "        x = self.fc2(x)            # Q-values for each action\n",
        "\n",
        "        return x\n",
        "\n",
        "# Test the network\n",
        "print(\"Testing Deep Q-Network...\")\n",
        "test_dqn = DQN(action_size=18)\n",
        "test_input = torch.randn(1, 4, 84, 84)  # Batch of 1, 4 frames, 84x84 pixels\n",
        "test_output = test_dqn(test_input)\n",
        "\n",
        "print(f\"Network created successfully!\")\n",
        "print(f\"Input shape: {test_input.shape} (1 sample, 4 frames, 84x84 pixels)\")\n",
        "print(f\"Output shape: {test_output.shape} (1 sample, 18 Q-values)\")\n",
        "print(f\"Sample Q-values: {test_output[0][:5].detach().numpy()}\")\n",
        "print(\"Neural network is ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1PsVK7x6E6o",
        "outputId": "57136b87-35c2-41e5-d58e-b42c4730060b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Deep Q-Network...\n",
            "Network created successfully!\n",
            "Input shape: torch.Size([1, 4, 84, 84]) (1 sample, 4 frames, 84x84 pixels)\n",
            "Output shape: torch.Size([1, 18]) (1 sample, 18 Q-values)\n",
            "Sample Q-values: [ 0.05254122 -0.00304434  0.04535846  0.02932352  0.02688643]\n",
            "Neural network is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Frame Preprocessor\n",
        "import cv2\n",
        "from collections import deque\n",
        "\n",
        "class FramePreprocessor:\n",
        "    \"\"\"\n",
        "    Preprocesses game frames for the DQN:\n",
        "    - Converts RGB to grayscale\n",
        "    - Resizes to 84x84\n",
        "    - Normalizes pixel values\n",
        "    - Stacks 4 frames together (gives agent sense of motion)\n",
        "    \"\"\"\n",
        "    def __init__(self, frame_stack_size=4):\n",
        "        self.frame_stack_size = frame_stack_size\n",
        "        self.frames = deque(maxlen=frame_stack_size)\n",
        "\n",
        "    def preprocess_frame(self, frame):\n",
        "        \"\"\"Convert a single frame to grayscale, resize, and normalize\"\"\"\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)\n",
        "        normalized = resized / 255.0  # Scale to 0-1 range\n",
        "        return normalized\n",
        "\n",
        "    def reset(self, initial_frame):\n",
        "        \"\"\"Initialize frame stack at episode start\"\"\"\n",
        "        processed = self.preprocess_frame(initial_frame)\n",
        "        self.frames.clear()\n",
        "        # Fill with 4 copies of the first frame\n",
        "        for _ in range(self.frame_stack_size):\n",
        "            self.frames.append(processed)\n",
        "        return np.stack(self.frames, axis=0)\n",
        "\n",
        "    def add_frame(self, frame):\n",
        "        \"\"\"Add a new frame to the stack\"\"\"\n",
        "        processed = self.preprocess_frame(frame)\n",
        "        self.frames.append(processed)\n",
        "        return np.stack(self.frames, axis=0)\n",
        "\n",
        "# Test the preprocessor\n",
        "print(\"Testing Frame Preprocessor...\")\n",
        "env = gym.make('ALE/BankHeist-v5', render_mode='rgb_array')\n",
        "state, _ = env.reset()\n",
        "\n",
        "preprocessor = FramePreprocessor()\n",
        "processed_state = preprocessor.reset(state)\n",
        "\n",
        "print(f\"Original frame shape: {state.shape}\")\n",
        "print(f\"Processed state shape: {processed_state.shape}\")\n",
        "print(f\"Pixel value range: [{processed_state.min():.3f}, {processed_state.max():.3f}]\")\n",
        "print(\"Frame preprocessor is ready!\")\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKPHiIbk7dSF",
        "outputId": "f3b2248d-726a-4b33-91ac-e60c2e9ea4a7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Frame Preprocessor...\n",
            "Original frame shape: (210, 160, 3)\n",
            "Processed state shape: (4, 84, 84)\n",
            "Pixel value range: [0.000, 0.580]\n",
            "Frame preprocessor is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replay Memory\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "class ReplayMemory:\n",
        "    \"\"\"\n",
        "    Experience Replay Buffer for DQN\n",
        "    Stores transitions (state, action, reward, next_state, done)\n",
        "    Samples random batches to break temporal correlation\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity=50000):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            capacity: Maximum number of experiences to store\n",
        "        \"\"\"\n",
        "        self.memory = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Store a transition in memory\"\"\"\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Sample a random batch of experiences\n",
        "\n",
        "        Returns:\n",
        "            Tuple of numpy arrays: (states, actions, rewards, next_states, dones)\n",
        "        \"\"\"\n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        return (\n",
        "            np.array(states),\n",
        "            np.array(actions),\n",
        "            np.array(rewards),\n",
        "            np.array(next_states),\n",
        "            np.array(dones)\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return current size of memory\"\"\"\n",
        "        return len(self.memory)\n",
        "\n",
        "# Test the replay memory\n",
        "print(\"Testing Replay Memory...\")\n",
        "memory = ReplayMemory(capacity=50000)\n",
        "\n",
        "# Add some dummy experiences\n",
        "for i in range(100):\n",
        "    dummy_state = np.random.rand(4, 84, 84)\n",
        "    dummy_action = np.random.randint(0, 18)\n",
        "    dummy_reward = np.random.rand()\n",
        "    dummy_next_state = np.random.rand(4, 84, 84)\n",
        "    dummy_done = False\n",
        "\n",
        "    memory.push(dummy_state, dummy_action, dummy_reward, dummy_next_state, dummy_done)\n",
        "\n",
        "print(f\"Memory size: {len(memory)}\")\n",
        "print(f\"Memory capacity: 50000\")\n",
        "\n",
        "# Sample a batch\n",
        "states, actions, rewards, next_states, dones = memory.sample(32)\n",
        "print(f\"Sampled batch - States shape: {states.shape}\")\n",
        "print(f\"Sampled batch - Actions shape: {actions.shape}\")\n",
        "print(f\"Sampled batch - Rewards shape: {rewards.shape}\")\n",
        "print(\"Replay memory is ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJBqNVvh7ogL",
        "outputId": "d890d86b-0014-475a-80b0-c557f55731eb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Replay Memory...\n",
            "Memory size: 100\n",
            "Memory capacity: 50000\n",
            "Sampled batch - States shape: (32, 4, 84, 84)\n",
            "Sampled batch - Actions shape: (32,)\n",
            "Sampled batch - Rewards shape: (32,)\n",
            "Replay memory is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DQN Agent\n",
        "import torch.optim as optim\n",
        "\n",
        "class DQNAgent:\n",
        "    \"\"\"\n",
        "    Deep Q-Network Agent for Bank Heist\n",
        "    Implements the DQN algorithm with experience replay and target network\n",
        "    \"\"\"\n",
        "    def __init__(self, state_shape, action_size, learning_rate=0.00025,\n",
        "                 gamma=0.99, epsilon_start=1.0, epsilon_min=0.01,\n",
        "                 epsilon_decay=0.995, memory_size=50000):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            state_shape: Shape of input state (4, 84, 84)\n",
        "            action_size: Number of possible actions (18 for Bank Heist)\n",
        "            learning_rate: Learning rate for optimizer (alpha in Bellman equation)\n",
        "            gamma: Discount factor for future rewards\n",
        "            epsilon_start: Initial exploration rate\n",
        "            epsilon_min: Minimum exploration rate\n",
        "            epsilon_decay: Rate at which epsilon decreases\n",
        "            memory_size: Capacity of replay buffer\n",
        "        \"\"\"\n",
        "        self.state_shape = state_shape\n",
        "        self.action_size = action_size\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Set device (GPU if available)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Agent using device: {self.device}\")\n",
        "\n",
        "        # Create policy network (the one we train)\n",
        "        self.policy_net = DQN(action_size).to(self.device)\n",
        "\n",
        "        # Create target network (stabilizes training)\n",
        "        self.target_net = DQN(action_size).to(self.device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()  # Set to evaluation mode\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = ReplayMemory(memory_size)\n",
        "\n",
        "    def select_action(self, state, training=True):\n",
        "        \"\"\"\n",
        "        Select action using epsilon-greedy policy\n",
        "\n",
        "        Args:\n",
        "            state: Current game state\n",
        "            training: If True, use epsilon-greedy; if False, use greedy\n",
        "\n",
        "        Returns:\n",
        "            Selected action (integer)\n",
        "        \"\"\"\n",
        "        # Exploration: random action\n",
        "        if training and random.random() < self.epsilon:\n",
        "            return random.randint(0, self.action_size - 1)\n",
        "\n",
        "        # Exploitation: best action according to Q-network\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "            q_values = self.policy_net(state_tensor)\n",
        "            return q_values.argmax().item()\n",
        "\n",
        "    def train_step(self, batch_size=32):\n",
        "        \"\"\"\n",
        "        Perform one training step using a batch from replay memory\n",
        "\n",
        "        Args:\n",
        "            batch_size: Number of experiences to sample\n",
        "\n",
        "        Returns:\n",
        "            Loss value for this training step\n",
        "        \"\"\"\n",
        "        # Need enough experiences in memory\n",
        "        if len(self.memory) < batch_size:\n",
        "            return None\n",
        "\n",
        "        # Sample batch from memory\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample(batch_size)\n",
        "\n",
        "        # Convert to tensors\n",
        "        states = torch.FloatTensor(states).to(self.device)\n",
        "        actions = torch.LongTensor(actions).to(self.device)\n",
        "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
        "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
        "        dones = torch.FloatTensor(dones).to(self.device)\n",
        "\n",
        "        # Current Q values: Q(s, a)\n",
        "        current_q_values = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
        "\n",
        "        # Next Q values from target network: max_a' Q_target(s', a')\n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.target_net(next_states).max(1)[0]\n",
        "            # Bellman equation: Q_target = r + gamma * max_a' Q(s', a') * (1 - done)\n",
        "            target_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n",
        "\n",
        "        # Compute loss (Mean Squared Error)\n",
        "        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n",
        "\n",
        "        # Optimize the network\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"Copy weights from policy network to target network\"\"\"\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"Decrease exploration rate\"\"\"\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def save(self, filepath):\n",
        "        \"\"\"Save model weights\"\"\"\n",
        "        torch.save({\n",
        "            'policy_net': self.policy_net.state_dict(),\n",
        "            'target_net': self.target_net.state_dict(),\n",
        "            'optimizer': self.optimizer.state_dict(),\n",
        "            'epsilon': self.epsilon\n",
        "        }, filepath)\n",
        "\n",
        "    def load(self, filepath):\n",
        "        \"\"\"Load model weights\"\"\"\n",
        "        checkpoint = torch.load(filepath)\n",
        "        self.policy_net.load_state_dict(checkpoint['policy_net'])\n",
        "        self.target_net.load_state_dict(checkpoint['target_net'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        self.epsilon = checkpoint['epsilon']\n",
        "\n",
        "# Test the agent\n",
        "print(\"Testing DQN Agent...\")\n",
        "test_agent = DQNAgent(state_shape=(4, 84, 84), action_size=18)\n",
        "test_state = np.random.rand(4, 84, 84)\n",
        "test_action = test_agent.select_action(test_state)\n",
        "print(f\"Agent selected action: {test_action}\")\n",
        "print(f\"Initial epsilon: {test_agent.epsilon}\")\n",
        "print(\"DQN Agent is ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQFP3Ink70tO",
        "outputId": "27906ff7-b740-431d-b749-1253aa399a68"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing DQN Agent...\n",
            "Agent using device: cuda\n",
            "Agent selected action: 13\n",
            "Initial epsilon: 1.0\n",
            "DQN Agent is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive and Setup Paths\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create directory in your Google Drive\n",
        "drive_base_path = '/content/drive/MyDrive/BankHeist_DQN_Project'\n",
        "os.makedirs(drive_base_path, exist_ok=True)\n",
        "\n",
        "print(f\"Google Drive mounted successfully!\")\n",
        "print(f\"Project directory: {drive_base_path}\")\n",
        "print(f\"All checkpoints, models, and metrics will be saved here\")\n",
        "print(\"-\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOHXxaI98Xer",
        "outputId": "56665c75-c2eb-4a85-ea5e-96e7084e1660"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully!\n",
            "Project directory: /content/drive/MyDrive/BankHeist_DQN_Project\n",
            "All checkpoints, models, and metrics will be saved here\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory-Efficient Trainer with Local + Drive Backup\n",
        "import time\n",
        "import json\n",
        "import sys\n",
        "import gc\n",
        "import shutil\n",
        "\n",
        "class BankHeistTrainer:\n",
        "    \"\"\"Memory-efficient trainer with auto-recovery\"\"\"\n",
        "    def __init__(self, save_dir='./bank_heist_checkpoints', drive_backup_dir=None):\n",
        "        self.save_dir = save_dir\n",
        "        self.drive_backup_dir = drive_backup_dir\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        if drive_backup_dir:\n",
        "            os.makedirs(drive_backup_dir, exist_ok=True)\n",
        "\n",
        "        self.episode_rewards = []\n",
        "        self.episode_steps = []\n",
        "        self.episode_losses = []\n",
        "        self.epsilon_history = []\n",
        "\n",
        "    def train(self, agent=None, start_episode=0, total_episodes=500,\n",
        "              max_steps=2000, batch_size=32, learning_rate=0.00025,\n",
        "              gamma=0.99, epsilon_start=1.0, epsilon_min=0.01,\n",
        "              epsilon_decay=0.995, memory_size=30000,  # REDUCED from 50000\n",
        "              target_update_freq=10, print_freq=50,\n",
        "              local_save_freq=50, drive_save_freq=200):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            local_save_freq: Save to local disk every N episodes (fast)\n",
        "            drive_save_freq: Copy to Drive every N episodes (slow but safe)\n",
        "        \"\"\"\n",
        "\n",
        "        env = gym.make('ALE/BankHeist-v5', render_mode='rgb_array')\n",
        "        preprocessor = FramePreprocessor()\n",
        "\n",
        "        if agent is None:\n",
        "            agent = DQNAgent(\n",
        "                state_shape=(4, 84, 84), action_size=18,\n",
        "                learning_rate=learning_rate, gamma=gamma,\n",
        "                epsilon_start=epsilon_start, epsilon_min=epsilon_min,\n",
        "                epsilon_decay=epsilon_decay, memory_size=memory_size\n",
        "            )\n",
        "            print(f\"New agent created | Memory: {memory_size}\")\n",
        "        else:\n",
        "            print(f\"Resuming from episode {start_episode}\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        for episode in range(start_episode, start_episode + total_episodes):\n",
        "            state, _ = env.reset()\n",
        "            state = preprocessor.reset(state)\n",
        "            episode_reward = 0\n",
        "            episode_loss = []\n",
        "\n",
        "            for step in range(max_steps):\n",
        "                action = agent.select_action(state, training=True)\n",
        "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated\n",
        "                next_state = preprocessor.add_frame(next_state)\n",
        "                agent.memory.push(state, action, reward, next_state, done)\n",
        "\n",
        "                loss = agent.train_step(batch_size)\n",
        "                if loss is not None:\n",
        "                    episode_loss.append(loss)\n",
        "\n",
        "                episode_reward += reward\n",
        "                state = next_state\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            if episode % target_update_freq == 0:\n",
        "                agent.update_target_network()\n",
        "            agent.decay_epsilon()\n",
        "\n",
        "            self.episode_rewards.append(episode_reward)\n",
        "            self.episode_steps.append(step + 1)\n",
        "            self.episode_losses.append(np.mean(episode_loss) if episode_loss else 0)\n",
        "            self.epsilon_history.append(agent.epsilon)\n",
        "\n",
        "            # FAST LOCAL SAVE (every 50 episodes)\n",
        "            if (episode + 1) % local_save_freq == 0:\n",
        "                self._save_local_checkpoint(agent, episode + 1)\n",
        "\n",
        "            # SLOW DRIVE BACKUP (every 200 episodes)\n",
        "            if self.drive_backup_dir and (episode + 1) % drive_save_freq == 0:\n",
        "                self._backup_to_drive()\n",
        "                print(f\"  Backed up to Drive at episode {episode + 1}\")\n",
        "                sys.stdout.flush()\n",
        "\n",
        "            # Print progress\n",
        "            if (episode + 1) % print_freq == 0:\n",
        "                avg_reward = np.mean(self.episode_rewards[-min(100, len(self.episode_rewards)):])\n",
        "                elapsed = time.time() - start_time\n",
        "                eps_done = episode - start_episode + 1\n",
        "                remaining = (elapsed/eps_done)*(total_episodes - eps_done)/60\n",
        "                print(f\"Ep {episode+1} | R:{episode_reward:.0f} | \"\n",
        "                      f\"Avg:{avg_reward:.1f} | ε:{agent.epsilon:.3f} | \"\n",
        "                      f\"Mem:{len(agent.memory)} | {remaining:.0f}m\")\n",
        "                sys.stdout.flush()\n",
        "\n",
        "        env.close()\n",
        "\n",
        "        # Final save to both local and Drive\n",
        "        final_checkpoint = f\"final_ep{start_episode + total_episodes}.pt\"\n",
        "        final_metrics = f\"metrics_ep{start_episode + total_episodes}.json\"\n",
        "\n",
        "        agent.save(f\"{self.save_dir}/{final_checkpoint}\")\n",
        "        self.save_metrics(f\"{self.save_dir}/{final_metrics}\")\n",
        "\n",
        "        if self.drive_backup_dir:\n",
        "            self._backup_to_drive()\n",
        "            print(\"Final backup to Drive complete\")\n",
        "\n",
        "        elapsed = (time.time()-start_time)/60\n",
        "        final_avg = np.mean(self.episode_rewards[-min(100, len(self.episode_rewards)):])\n",
        "        print(f\"\\nComplete: {elapsed:.1f}m | Avg:{final_avg:.1f} | ε:{agent.epsilon:.3f}\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        return agent\n",
        "\n",
        "    def _save_local_checkpoint(self, agent, episode):\n",
        "        \"\"\"Quick save to local disk\"\"\"\n",
        "        agent.save(f\"{self.save_dir}/checkpoint_ep{episode}.pt\")\n",
        "        self.save_metrics(f\"{self.save_dir}/metrics_ep{episode}.json\")\n",
        "\n",
        "    def _backup_to_drive(self):\n",
        "        \"\"\"Copy all local files to Drive\"\"\"\n",
        "        if self.drive_backup_dir:\n",
        "            for filename in os.listdir(self.save_dir):\n",
        "                src = os.path.join(self.save_dir, filename)\n",
        "                dst = os.path.join(self.drive_backup_dir, filename)\n",
        "                shutil.copy2(src, dst)\n",
        "\n",
        "    def save_metrics(self, filepath):\n",
        "        metrics = {\n",
        "            'episode_rewards': self.episode_rewards,\n",
        "            'episode_steps': self.episode_steps,\n",
        "            'episode_losses': self.episode_losses,\n",
        "            'epsilon_history': self.epsilon_history,\n",
        "            'total_episodes': len(self.episode_rewards),\n",
        "            'avg_reward_last_100': float(np.mean(self.episode_rewards[-min(100, len(self.episode_rewards)):])),\n",
        "            'avg_steps_last_100': float(np.mean(self.episode_steps[-min(100, len(self.episode_steps)):]))\n",
        "        }\n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump(metrics, f, indent=2)\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Free memory after experiment\"\"\"\n",
        "        self.episode_rewards.clear()\n",
        "        self.episode_steps.clear()\n",
        "        self.episode_losses.clear()\n",
        "        self.epsilon_history.clear()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"Memory cleared\")\n",
        "\n",
        "print(\"Memory-efficient trainer ready\")"
      ],
      "metadata": {
        "id": "wP_XbkkHBUTx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "726ddae9-4b57-4665-c44e-ec655d6a9e56"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory-efficient trainer ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Checkpoint and Resume Training\n",
        "def load_checkpoint_and_resume(checkpoint_path, trainer):\n",
        "    \"\"\"Load a checkpoint and return agent + last episode number\"\"\"\n",
        "    agent = DQNAgent(\n",
        "        state_shape=(4, 84, 84), action_size=18,\n",
        "        learning_rate=0.00025, gamma=0.99,\n",
        "        epsilon_start=1.0, epsilon_min=0.01,\n",
        "        epsilon_decay=0.995, memory_size=30000\n",
        "    )\n",
        "    agent.load(checkpoint_path)\n",
        "\n",
        "    # Extract episode number from filename\n",
        "    import re\n",
        "    match = re.search(r'ep(\\d+)', checkpoint_path)\n",
        "    last_episode = int(match.group(1)) if match else 0\n",
        "\n",
        "    print(f\"Loaded checkpoint from episode {last_episode}\")\n",
        "    print(f\"Agent epsilon: {agent.epsilon}\")\n",
        "\n",
        "    return agent, last_episode\n",
        "\n",
        "# Example usage if you need to resume:\n",
        "# agent, last_ep = load_checkpoint_and_resume('./local_baseline/checkpoint_ep400.pt', trainer_baseline)\n",
        "# Then continue training from last_ep\n",
        "\n",
        "print(\"Recovery helper ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d32GzNM4BaqF",
        "outputId": "e7591e1b-c811-42b2-d9cf-2f6656d16c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recovery helper ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline Batch 1 (Episodes 1-400)\n",
        "local_dir = './local_baseline'\n",
        "drive_dir = f\"{drive_base_path}/baseline_run\"\n",
        "\n",
        "trainer_baseline = BankHeistTrainer(\n",
        "    save_dir=local_dir,\n",
        "    drive_backup_dir=drive_dir\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"BASELINE - BATCH 1: Episodes 1-400\")\n",
        "print(\"Saving locally every 50 eps, Drive backup every 200 eps\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "agent_baseline = trainer_baseline.train(\n",
        "    agent=None,\n",
        "    start_episode=0,\n",
        "    total_episodes=400,\n",
        "    max_steps=2000,\n",
        "    batch_size=32,\n",
        "    learning_rate=0.00025,\n",
        "    gamma=0.99,\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_min=0.01,\n",
        "    epsilon_decay=0.995,\n",
        "    memory_size=30000,\n",
        "    target_update_freq=10,\n",
        "    print_freq=50,\n",
        "    local_save_freq=50,\n",
        "    drive_save_freq=200\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArrRzkNGBc6u",
        "outputId": "751a0d41-96b9-45d9-9805-0b5ea0a88040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "BASELINE - BATCH 1: Episodes 1-400\n",
            "Saving locally every 50 eps, Drive backup every 200 eps\n",
            "============================================================\n",
            "Agent using device: cuda\n",
            "New agent created | Memory: 30000\n",
            "Ep 50 | R:10 | Avg:15.2 | ε:0.778 | Mem:26609 | 31m\n",
            "Ep 100 | R:80 | Avg:14.9 | ε:0.606 | Mem:30000 | 26m\n",
            "Ep 150 | R:20 | Avg:17.1 | ε:0.471 | Mem:30000 | 22m\n",
            "  Backed up to Drive at episode 200\n",
            "Ep 200 | R:10 | Avg:21.8 | ε:0.367 | Mem:30000 | 18m\n",
            "Ep 250 | R:20 | Avg:23.7 | ε:0.286 | Mem:30000 | 14m\n",
            "Ep 300 | R:20 | Avg:26.8 | ε:0.222 | Mem:30000 | 9m\n",
            "Ep 350 | R:20 | Avg:29.3 | ε:0.173 | Mem:30000 | 5m\n",
            "  Backed up to Drive at episode 400\n",
            "Ep 400 | R:10 | Avg:27.9 | ε:0.135 | Mem:30000 | 0m\n",
            "Final backup to Drive complete\n",
            "\n",
            "Complete: 37.2m | Avg:27.9 | ε:0.135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick test - see how long episodes actually last\n",
        "env = gym.make('ALE/BankHeist-v5', render_mode='rgb_array')\n",
        "preprocessor = FramePreprocessor()\n",
        "\n",
        "print(\"Testing 10 episodes to see natural length...\")\n",
        "step_counts = []\n",
        "\n",
        "for i in range(10):\n",
        "    state, _ = env.reset()\n",
        "    state = preprocessor.reset(state)\n",
        "\n",
        "    for step in range(5000):  # High limit\n",
        "        action = env.action_space.sample()  # Random actions\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "        if terminated or truncated:\n",
        "            step_counts.append(step + 1)\n",
        "            print(f\"Episode {i+1}: {step+1} steps\")\n",
        "            break\n",
        "\n",
        "env.close()\n",
        "\n",
        "print(f\"\\nAverage episode length: {np.mean(step_counts):.0f} steps\")\n",
        "print(f\"Max episode length: {max(step_counts)} steps\")\n",
        "print(f\"Min episode length: {min(step_counts)} steps\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjOsgh1hJd9Q",
        "outputId": "b174e6c3-e03e-4ec7-ccd8-a64f134d0d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing 10 episodes to see natural length...\n",
            "Episode 1: 838 steps\n",
            "Episode 2: 668 steps\n",
            "Episode 3: 388 steps\n",
            "Episode 4: 584 steps\n",
            "Episode 5: 495 steps\n",
            "Episode 6: 808 steps\n",
            "Episode 7: 699 steps\n",
            "Episode 8: 443 steps\n",
            "Episode 9: 656 steps\n",
            "Episode 10: 364 steps\n",
            "\n",
            "Average episode length: 594 steps\n",
            "Max episode length: 838 steps\n",
            "Min episode length: 364 steps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline Batch 2 (Episodes 401-800)\n",
        "print(\"=\" * 60)\n",
        "print(\"BASELINE - BATCH 2: Episodes 401-800\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "agent_baseline = trainer_baseline.train(\n",
        "    agent=agent_baseline,\n",
        "    start_episode=400,\n",
        "    total_episodes=400,\n",
        "    max_steps=2000,\n",
        "    batch_size=32,\n",
        "    print_freq=50,\n",
        "    local_save_freq=50,\n",
        "    drive_save_freq=200\n",
        ")\n",
        "\n",
        "# Final save and cleanup\n",
        "agent_baseline.save(f\"{drive_dir}/final_baseline.pt\")\n",
        "trainer_baseline.save_metrics(f\"{drive_dir}/final_baseline_metrics.json\")\n",
        "print(\"Baseline complete!\")\n",
        "\n",
        "# Free memory before next experiment\n",
        "del agent_baseline\n",
        "trainer_baseline.cleanup()\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"Memory freed for next experiment\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsTnIkugBe1I",
        "outputId": "e7e04aee-5f30-47d0-b8a3-7fcfbb7fc26a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "BASELINE - BATCH 2: Episodes 401-800\n",
            "============================================================\n",
            "Resuming from episode 400\n",
            "Ep 450 | R:40 | Avg:33.5 | ε:0.096 | Mem:30000 | 36m\n",
            "Ep 500 | R:10 | Avg:37.7 | ε:0.075 | Mem:30000 | 33m\n",
            "Ep 550 | R:50 | Avg:41.9 | ε:0.058 | Mem:30000 | 27m\n",
            "  Backed up to Drive at episode 600\n",
            "Ep 600 | R:80 | Avg:62.9 | ε:0.045 | Mem:30000 | 22m\n",
            "Ep 650 | R:110 | Avg:89.8 | ε:0.035 | Mem:30000 | 17m\n",
            "Ep 700 | R:200 | Avg:137.1 | ε:0.027 | Mem:30000 | 12m\n",
            "Ep 750 | R:190 | Avg:195.2 | ε:0.021 | Mem:30000 | 6m\n",
            "  Backed up to Drive at episode 800\n",
            "Ep 800 | R:90 | Avg:215.7 | ε:0.017 | Mem:30000 | 0m\n",
            "Final backup to Drive complete\n",
            "\n",
            "Complete: 52.6m | Avg:215.7 | ε:0.017\n",
            "Baseline complete!\n",
            "Memory cleared\n",
            "Memory freed for next experiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reconnect after crash\n",
        "from google.colab import drive\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "drive_base_path = '/content/drive/MyDrive/BankHeist_DQN_Project'\n",
        "\n",
        "# Verify baseline is saved\n",
        "print(\"Checking saved baseline...\")\n",
        "with open(f\"{drive_base_path}/baseline_run/final_baseline_metrics.json\", 'r') as f:\n",
        "    baseline_metrics = json.load(f)\n",
        "\n",
        "print(f\"Baseline (800 eps): {baseline_metrics['avg_reward_last_100']:.1f} points\")\n",
        "print(\"Ready to start experiments!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOalyLNIzB9a",
        "outputId": "4243ca63-3a84-4937-bbfa-271e37aa8dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Checking saved baseline...\n",
            "Baseline (800 eps): 215.7 points\n",
            "Ready to start experiments!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 1 - Higher Learning Rate (α=0.0005)\n",
        "import gc\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "local_dir_exp1 = './local_exp1'\n",
        "drive_dir_exp1 = f\"{drive_base_path}/experiment1_alpha\"\n",
        "\n",
        "trainer_exp1 = BankHeistTrainer(\n",
        "    save_dir=local_dir_exp1,\n",
        "    drive_backup_dir=drive_dir_exp1\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EXPERIMENT 1: Higher Alpha (α=0.0005 vs baseline 0.00025)\")\n",
        "print(\"Testing if faster learning improves performance\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "agent_exp1 = trainer_exp1.train(\n",
        "    agent=None,\n",
        "    start_episode=0,\n",
        "    total_episodes=400,\n",
        "    max_steps=2000,\n",
        "    batch_size=32,\n",
        "    learning_rate=0.0005,\n",
        "    gamma=0.99,\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_min=0.01,\n",
        "    epsilon_decay=0.995,\n",
        "    memory_size=30000,\n",
        "    target_update_freq=10,\n",
        "    print_freq=50,\n",
        "    local_save_freq=50,\n",
        "    drive_save_freq=200\n",
        ")\n",
        "\n",
        "agent_exp1.save(f\"{drive_dir_exp1}/final_exp1.pt\")\n",
        "trainer_exp1.save_metrics(f\"{drive_dir_exp1}/final_exp1_metrics.json\")\n",
        "\n",
        "del agent_exp1\n",
        "trainer_exp1.cleanup()\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"Experiment 1 complete, memory cleared\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T4YkoED4boo",
        "outputId": "c83214c4-b890-4461-a677-280d2cbac0d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "EXPERIMENT 1: Higher Alpha (α=0.0005 vs baseline 0.00025)\n",
            "Testing if faster learning improves performance\n",
            "============================================================\n",
            "Agent using device: cuda\n",
            "New agent created | Memory: 30000\n",
            "Ep 50 | R:30 | Avg:14.2 | ε:0.778 | Mem:27289 | 31m\n",
            "Ep 100 | R:30 | Avg:13.0 | ε:0.606 | Mem:30000 | 27m\n",
            "Ep 150 | R:0 | Avg:11.6 | ε:0.471 | Mem:30000 | 24m\n",
            "  Backed up to Drive at episode 200\n",
            "Ep 200 | R:20 | Avg:11.8 | ε:0.367 | Mem:30000 | 20m\n",
            "Ep 250 | R:10 | Avg:11.7 | ε:0.286 | Mem:30000 | 16m\n",
            "Ep 300 | R:40 | Avg:10.3 | ε:0.222 | Mem:30000 | 11m\n",
            "Ep 350 | R:90 | Avg:9.4 | ε:0.173 | Mem:30000 | 6m\n",
            "  Backed up to Drive at episode 400\n",
            "Ep 400 | R:0 | Avg:8.8 | ε:0.135 | Mem:30000 | 0m\n",
            "Final backup to Drive complete\n",
            "\n",
            "Complete: 47.3m | Avg:8.8 | ε:0.135\n",
            "Memory cleared\n",
            "Experiment 1 complete, memory cleared\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extend Baseline - Episodes 801-1200\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EXTENDING BASELINE: Episodes 801-1200\")\n",
        "print(\"Goal: Show continued improvement beyond 215.7\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Setup paths\n",
        "local_dir = './local_baseline'\n",
        "drive_dir = f\"{drive_base_path}/baseline_run\"\n",
        "\n",
        "# Recreate trainer (memory was cleared earlier)\n",
        "trainer_baseline = BankHeistTrainer(\n",
        "    save_dir=local_dir,\n",
        "    drive_backup_dir=drive_dir\n",
        ")\n",
        "\n",
        "# Create agent and load saved weights from episode 800\n",
        "agent_baseline = DQNAgent(\n",
        "    state_shape=(4, 84, 84),\n",
        "    action_size=18,\n",
        "    learning_rate=0.00025,\n",
        "    gamma=0.99,\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_min=0.01,\n",
        "    epsilon_decay=0.995,\n",
        "    memory_size=30000\n",
        ")\n",
        "\n",
        "# Load the checkpoint\n",
        "agent_baseline.load(f\"{drive_dir}/final_baseline.pt\")\n",
        "print(f\"Loaded baseline from episode 800\")\n",
        "print(f\"Starting epsilon: {agent_baseline.epsilon:.4f}\")\n",
        "print(f\"Starting from avg: 215.7 points\")\n",
        "print()\n",
        "\n",
        "# Continue training for 400 more episodes\n",
        "agent_baseline = trainer_baseline.train(\n",
        "    agent=agent_baseline,\n",
        "    start_episode=800,\n",
        "    total_episodes=400,\n",
        "    max_steps=2000,\n",
        "    batch_size=32,\n",
        "    print_freq=50,\n",
        "    local_save_freq=50,\n",
        "    drive_save_freq=200\n",
        ")\n",
        "\n",
        "# Save extended baseline\n",
        "agent_baseline.save(f\"{drive_dir}/baseline_1200ep.pt\")\n",
        "trainer_baseline.save_metrics(f\"{drive_dir}/baseline_1200ep_metrics.json\")\n",
        "\n",
        "# Calculate final average\n",
        "final_avg = np.mean(trainer_baseline.episode_rewards[-100:])\n",
        "\n",
        "print()\n",
        "print(\"=\" * 60)\n",
        "print(\"BASELINE EXTENSION COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Episodes 800 → 1200\")\n",
        "print(f\"Starting average: 215.7\")\n",
        "print(f\"Final average: {final_avg:.1f}\")\n",
        "print(f\"Improvement: {final_avg - 215.7:+.1f} points ({((final_avg/215.7)-1)*100:+.1f}%)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# CLEAR MEMORY\n",
        "del agent_baseline\n",
        "trainer_baseline.cleanup()\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"\\nMemory cleared - ready for experiments!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pFEoDuSElOU",
        "outputId": "04268f4c-08b9-4a17-c237-cc7e04a06688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "EXTENDING BASELINE: Episodes 801-1200\n",
            "Goal: Show continued improvement beyond 215.7\n",
            "============================================================\n",
            "Agent using device: cuda\n",
            "Loaded baseline from episode 800\n",
            "Starting epsilon: 0.0167\n",
            "Starting from avg: 215.7 points\n",
            "\n",
            "Resuming from episode 800\n",
            "Ep 850 | R:470 | Avg:335.4 | ε:0.013 | Mem:30000 | 61m\n",
            "Ep 900 | R:390 | Avg:359.2 | ε:0.010 | Mem:30000 | 55m\n",
            "Ep 950 | R:290 | Avg:433.5 | ε:0.010 | Mem:30000 | 48m\n",
            "  Backed up to Drive at episode 1000\n",
            "Ep 1000 | R:470 | Avg:493.0 | ε:0.010 | Mem:30000 | 40m\n",
            "Ep 1050 | R:210 | Avg:483.3 | ε:0.010 | Mem:30000 | 30m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load baseline\n",
        "demo_agent = DQNAgent(\n",
        "    state_shape=(4, 84, 84),\n",
        "    action_size=18,\n",
        "    learning_rate=0.00025,\n",
        "    gamma=0.99,\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_min=0.01,\n",
        "    epsilon_decay=0.995,\n",
        "    memory_size=30000\n",
        ")\n",
        "\n",
        "checkpoint_path = f\"{drive_base_path}/baseline_run/checkpoint_ep1000.pt\"\n",
        "demo_agent.load(checkpoint_path)\n",
        "print(f\"Epsilon: {demo_agent.epsilon:.4f}\")\n",
        "print()\n",
        "\n",
        "# Environment\n",
        "env = gym.make('ALE/BankHeist-v5', render_mode='rgb_array')\n",
        "preprocessor = FramePreprocessor()\n",
        "\n",
        "# Record ONE short episode\n",
        "print(\"Recording ONE episode (first 200 steps only)...\")\n",
        "state, _ = env.reset()\n",
        "state = preprocessor.reset(state)\n",
        "\n",
        "frames = []\n",
        "scores = []\n",
        "episode_reward = 0\n",
        "\n",
        "for step in range(200):  # Just 200 steps\n",
        "    frame = env.render()\n",
        "\n",
        "    # DUPLICATE 5 TIMES for very slow playback\n",
        "    for _ in range(5):\n",
        "        frames.append(frame.copy())\n",
        "        scores.append(episode_reward)\n",
        "\n",
        "    # Agent action\n",
        "    action = demo_agent.select_action(state, training=True)\n",
        "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "    if reward > 0:\n",
        "        print(f\"  Step {step}: +{reward:.0f} points!\")\n",
        "\n",
        "    episode_reward += reward\n",
        "    state = preprocessor.add_frame(next_state)\n",
        "\n",
        "    if terminated or truncated:\n",
        "        print(f\"  Episode ended at step {step}\")\n",
        "        break\n",
        "\n",
        "env.close()\n",
        "\n",
        "video_duration = len(frames) / 20 / 60\n",
        "\n",
        "print()\n",
        "print(\"=\" * 60)\n",
        "print(f\"Frames: {len(frames)} (5x slower)\")\n",
        "print(f\"Final score: {episode_reward:.0f}\")\n",
        "print(f\"Video duration: ~{video_duration:.1f} minutes\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "# Create video\n",
        "print(\"Creating video...\")\n",
        "fig, ax = plt.subplots(figsize=(10, 7))\n",
        "ax.axis('off')\n",
        "\n",
        "title = fig.text(0.5, 0.95,\n",
        "                 \"Bank Heist\",\n",
        "                 ha='center', fontsize=12, weight='bold')\n",
        "\n",
        "img = ax.imshow(frames[0])\n",
        "\n",
        "score_text = ax.text(\n",
        "    0.95, 0.90, '',\n",
        "    transform=ax.transAxes,\n",
        "    verticalalignment='top',\n",
        "    horizontalalignment='right',\n",
        "    fontsize=16,\n",
        "    bbox=dict(boxstyle='round,pad=0.5', facecolor='black', alpha=0.8),\n",
        "    color='lime',\n",
        "    weight='bold'\n",
        ")\n",
        "\n",
        "def animate(frame_idx):\n",
        "    img.set_array(frames[frame_idx])\n",
        "    score_text.set_text(f'Score: {scores[frame_idx]:.0f}')\n",
        "    return [img, score_text]\n",
        "\n",
        "anim = animation.FuncAnimation(fig, animate, frames=len(frames),\n",
        "                               interval=50, blit=True, repeat=False)\n",
        "\n",
        "video_path = f'{drive_base_path}/gameplay_ultraslow.mp4'\n",
        "anim.save(video_path, writer='ffmpeg', fps=20, dpi=100, bitrate=2000)\n",
        "plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlY2pr7T9pa9",
        "outputId": "1f558d8b-18c8-47f9-976c-f85fe850e107"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Agent using device: cuda\n",
            "Epsilon: 0.0100\n",
            "\n",
            "Recording ONE episode (first 200 steps only)...\n",
            "  Step 7: +10 points!\n",
            "  Step 26: +10 points!\n",
            "  Step 42: +10 points!\n",
            "  Step 57: +10 points!\n",
            "  Step 77: +10 points!\n",
            "  Step 91: +10 points!\n",
            "  Step 104: +10 points!\n",
            "  Step 119: +10 points!\n",
            "  Step 135: +10 points!\n",
            "  Step 150: +10 points!\n",
            "  Step 167: +10 points!\n",
            "  Step 181: +10 points!\n",
            "  Step 195: +10 points!\n",
            "\n",
            "============================================================\n",
            "Frames: 1000 (5x slower)\n",
            "Final score: 130\n",
            "Video duration: ~0.8 minutes\n",
            "============================================================\n",
            "\n",
            "Creating video...\n"
          ]
        }
      ]
    }
  ]
}