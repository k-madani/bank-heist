{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install Required Packages\n",
        "!pip install gymnasium[atari] -q\n",
        "!pip install gymnasium[accept-rom-license] -q\n",
        "!pip install ale-py -q\n",
        "!pip install torch torchvision -q\n",
        "!pip install opencv-python -q\n",
        "!pip install matplotlib -q\n",
        "\n",
        "print(\"All packages installed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UxPsYGs5Owc",
        "outputId": "47b33830-599f-4abc-ffe0-91fa71f70d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: gymnasium 1.2.2 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mAll packages installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Environment and GPU\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import numpy as np\n",
        "import ale_py\n",
        "\n",
        "# Register Atari environments\n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "# Check GPU availability\n",
        "print(\"=\" * 50)\n",
        "print(\"SYSTEM CHECK\")\n",
        "print(\"=\" * 50)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "print()\n",
        "\n",
        "# Test Bank Heist environment\n",
        "print(\"=\" * 50)\n",
        "print(\"TESTING BANK HEIST ENVIRONMENT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "env = gym.make('ALE/BankHeist-v5', render_mode='rgb_array')\n",
        "state, info = env.reset()\n",
        "\n",
        "print(f\"Environment created successfully!\")\n",
        "print(f\"State shape (frame dimensions): {state.shape}\")\n",
        "print(f\"Number of possible actions: {env.action_space.n}\")\n",
        "print(f\"Actions available: {env.unwrapped.get_action_meanings()}\")\n",
        "\n",
        "# Take a test action\n",
        "action = env.action_space.sample()\n",
        "next_state, reward, terminated, truncated, info = env.step(action)\n",
        "print(f\"Test action completed - Reward: {reward}\")\n",
        "\n",
        "env.close()\n",
        "print()\n",
        "print(\"All tests passed! Ready to build the agent!\")\n",
        "print(\"=\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCKyvmkK5lrw",
        "outputId": "1205980b-c7d6-4929-fe8f-fd81bc69d3fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "SYSTEM CHECK\n",
            "==================================================\n",
            "Using device: cuda\n",
            "GPU Name: Tesla T4\n",
            "GPU Memory: 15.83 GB\n",
            "\n",
            "==================================================\n",
            "TESTING BANK HEIST ENVIRONMENT\n",
            "==================================================\n",
            "Environment created successfully!\n",
            "State shape (frame dimensions): (210, 160, 3)\n",
            "Number of possible actions: 18\n",
            "Actions available: ['NOOP', 'FIRE', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT', 'UPFIRE', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n",
            "Test action completed - Reward: 0.0\n",
            "\n",
            "All tests passed! Ready to build the agent!\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3: Build the Deep Q-Network\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    \"\"\"\n",
        "    Deep Q-Network for Bank Heist\n",
        "    Takes 4 stacked grayscale frames (84x84) as input\n",
        "    Outputs Q-values for each of the 18 possible actions\n",
        "    \"\"\"\n",
        "    def __init__(self, action_size):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        # Convolutional layers to process game frames\n",
        "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)  # 4 frames -> 32 features\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2) # 32 -> 64 features\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1) # 64 -> 64 features\n",
        "\n",
        "        # Fully connected layers for decision making\n",
        "        self.fc1 = nn.Linear(7 * 7 * 64, 512)  # Flatten and connect\n",
        "        self.fc2 = nn.Linear(512, action_size)  # Output Q-value for each action\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Process frames through the network\"\"\"\n",
        "        x = F.relu(self.conv1(x))  # Learn features from frames\n",
        "        x = F.relu(self.conv2(x))  # Learn higher-level features\n",
        "        x = F.relu(self.conv3(x))  # Learn even higher-level features\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten for fully connected layers\n",
        "\n",
        "        x = F.relu(self.fc1(x))    # Decision making layer\n",
        "        x = self.fc2(x)            # Q-values for each action\n",
        "\n",
        "        return x\n",
        "\n",
        "# Test the network\n",
        "print(\"Testing Deep Q-Network...\")\n",
        "test_dqn = DQN(action_size=18)\n",
        "test_input = torch.randn(1, 4, 84, 84)  # Batch of 1, 4 frames, 84x84 pixels\n",
        "test_output = test_dqn(test_input)\n",
        "\n",
        "print(f\"Network created successfully!\")\n",
        "print(f\"Input shape: {test_input.shape} (1 sample, 4 frames, 84x84 pixels)\")\n",
        "print(f\"Output shape: {test_output.shape} (1 sample, 18 Q-values)\")\n",
        "print(f\"Sample Q-values: {test_output[0][:5].detach().numpy()}\")\n",
        "print(\"Neural network is ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1PsVK7x6E6o",
        "outputId": "41c86faf-0130-47e5-a2bb-6d937410ee6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Deep Q-Network...\n",
            "Network created successfully!\n",
            "Input shape: torch.Size([1, 4, 84, 84]) (1 sample, 4 frames, 84x84 pixels)\n",
            "Output shape: torch.Size([1, 18]) (1 sample, 18 Q-values)\n",
            "Sample Q-values: [-0.01339139  0.00270121  0.00952767 -0.05176202 -0.02827951]\n",
            "Neural network is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Frame Preprocessor\n",
        "import cv2\n",
        "from collections import deque\n",
        "\n",
        "class FramePreprocessor:\n",
        "    \"\"\"\n",
        "    Preprocesses game frames for the DQN:\n",
        "    - Converts RGB to grayscale\n",
        "    - Resizes to 84x84\n",
        "    - Normalizes pixel values\n",
        "    - Stacks 4 frames together (gives agent sense of motion)\n",
        "    \"\"\"\n",
        "    def __init__(self, frame_stack_size=4):\n",
        "        self.frame_stack_size = frame_stack_size\n",
        "        self.frames = deque(maxlen=frame_stack_size)\n",
        "\n",
        "    def preprocess_frame(self, frame):\n",
        "        \"\"\"Convert a single frame to grayscale, resize, and normalize\"\"\"\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)\n",
        "        normalized = resized / 255.0  # Scale to 0-1 range\n",
        "        return normalized\n",
        "\n",
        "    def reset(self, initial_frame):\n",
        "        \"\"\"Initialize frame stack at episode start\"\"\"\n",
        "        processed = self.preprocess_frame(initial_frame)\n",
        "        self.frames.clear()\n",
        "        # Fill with 4 copies of the first frame\n",
        "        for _ in range(self.frame_stack_size):\n",
        "            self.frames.append(processed)\n",
        "        return np.stack(self.frames, axis=0)\n",
        "\n",
        "    def add_frame(self, frame):\n",
        "        \"\"\"Add a new frame to the stack\"\"\"\n",
        "        processed = self.preprocess_frame(frame)\n",
        "        self.frames.append(processed)\n",
        "        return np.stack(self.frames, axis=0)\n",
        "\n",
        "# Test the preprocessor\n",
        "print(\"Testing Frame Preprocessor...\")\n",
        "env = gym.make('ALE/BankHeist-v5', render_mode='rgb_array')\n",
        "state, _ = env.reset()\n",
        "\n",
        "preprocessor = FramePreprocessor()\n",
        "processed_state = preprocessor.reset(state)\n",
        "\n",
        "print(f\"Original frame shape: {state.shape}\")\n",
        "print(f\"Processed state shape: {processed_state.shape}\")\n",
        "print(f\"Pixel value range: [{processed_state.min():.3f}, {processed_state.max():.3f}]\")\n",
        "print(\"Frame preprocessor is ready!\")\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKPHiIbk7dSF",
        "outputId": "88e31e25-43d0-4d9d-8b15-b2ff2ac97406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Frame Preprocessor...\n",
            "Original frame shape: (210, 160, 3)\n",
            "Processed state shape: (4, 84, 84)\n",
            "Pixel value range: [0.000, 0.580]\n",
            "Frame preprocessor is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replay Memory\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "class ReplayMemory:\n",
        "    \"\"\"\n",
        "    Experience Replay Buffer for DQN\n",
        "    Stores transitions (state, action, reward, next_state, done)\n",
        "    Samples random batches to break temporal correlation\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity=50000):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            capacity: Maximum number of experiences to store\n",
        "        \"\"\"\n",
        "        self.memory = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Store a transition in memory\"\"\"\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Sample a random batch of experiences\n",
        "\n",
        "        Returns:\n",
        "            Tuple of numpy arrays: (states, actions, rewards, next_states, dones)\n",
        "        \"\"\"\n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        return (\n",
        "            np.array(states),\n",
        "            np.array(actions),\n",
        "            np.array(rewards),\n",
        "            np.array(next_states),\n",
        "            np.array(dones)\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return current size of memory\"\"\"\n",
        "        return len(self.memory)\n",
        "\n",
        "# Test the replay memory\n",
        "print(\"Testing Replay Memory...\")\n",
        "memory = ReplayMemory(capacity=50000)\n",
        "\n",
        "# Add some dummy experiences\n",
        "for i in range(100):\n",
        "    dummy_state = np.random.rand(4, 84, 84)\n",
        "    dummy_action = np.random.randint(0, 18)\n",
        "    dummy_reward = np.random.rand()\n",
        "    dummy_next_state = np.random.rand(4, 84, 84)\n",
        "    dummy_done = False\n",
        "\n",
        "    memory.push(dummy_state, dummy_action, dummy_reward, dummy_next_state, dummy_done)\n",
        "\n",
        "print(f\"Memory size: {len(memory)}\")\n",
        "print(f\"Memory capacity: 50000\")\n",
        "\n",
        "# Sample a batch\n",
        "states, actions, rewards, next_states, dones = memory.sample(32)\n",
        "print(f\"Sampled batch - States shape: {states.shape}\")\n",
        "print(f\"Sampled batch - Actions shape: {actions.shape}\")\n",
        "print(f\"Sampled batch - Rewards shape: {rewards.shape}\")\n",
        "print(\"Replay memory is ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJBqNVvh7ogL",
        "outputId": "fd9e057c-0868-437f-8e72-57629a07d20d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Replay Memory...\n",
            "Memory size: 100\n",
            "Memory capacity: 50000\n",
            "Sampled batch - States shape: (32, 4, 84, 84)\n",
            "Sampled batch - Actions shape: (32,)\n",
            "Sampled batch - Rewards shape: (32,)\n",
            "Replay memory is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DQN Agent\n",
        "import torch.optim as optim\n",
        "\n",
        "class DQNAgent:\n",
        "    \"\"\"\n",
        "    Deep Q-Network Agent for Bank Heist\n",
        "    Implements the DQN algorithm with experience replay and target network\n",
        "    \"\"\"\n",
        "    def __init__(self, state_shape, action_size, learning_rate=0.00025,\n",
        "                 gamma=0.99, epsilon_start=1.0, epsilon_min=0.01,\n",
        "                 epsilon_decay=0.995, memory_size=50000):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            state_shape: Shape of input state (4, 84, 84)\n",
        "            action_size: Number of possible actions (18 for Bank Heist)\n",
        "            learning_rate: Learning rate for optimizer (alpha in Bellman equation)\n",
        "            gamma: Discount factor for future rewards\n",
        "            epsilon_start: Initial exploration rate\n",
        "            epsilon_min: Minimum exploration rate\n",
        "            epsilon_decay: Rate at which epsilon decreases\n",
        "            memory_size: Capacity of replay buffer\n",
        "        \"\"\"\n",
        "        self.state_shape = state_shape\n",
        "        self.action_size = action_size\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Set device (GPU if available)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Agent using device: {self.device}\")\n",
        "\n",
        "        # Create policy network (the one we train)\n",
        "        self.policy_net = DQN(action_size).to(self.device)\n",
        "\n",
        "        # Create target network (stabilizes training)\n",
        "        self.target_net = DQN(action_size).to(self.device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()  # Set to evaluation mode\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = ReplayMemory(memory_size)\n",
        "\n",
        "    def select_action(self, state, training=True):\n",
        "        \"\"\"\n",
        "        Select action using epsilon-greedy policy\n",
        "\n",
        "        Args:\n",
        "            state: Current game state\n",
        "            training: If True, use epsilon-greedy; if False, use greedy\n",
        "\n",
        "        Returns:\n",
        "            Selected action (integer)\n",
        "        \"\"\"\n",
        "        # Exploration: random action\n",
        "        if training and random.random() < self.epsilon:\n",
        "            return random.randint(0, self.action_size - 1)\n",
        "\n",
        "        # Exploitation: best action according to Q-network\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "            q_values = self.policy_net(state_tensor)\n",
        "            return q_values.argmax().item()\n",
        "\n",
        "    def train_step(self, batch_size=32):\n",
        "        \"\"\"\n",
        "        Perform one training step using a batch from replay memory\n",
        "\n",
        "        Args:\n",
        "            batch_size: Number of experiences to sample\n",
        "\n",
        "        Returns:\n",
        "            Loss value for this training step\n",
        "        \"\"\"\n",
        "        # Need enough experiences in memory\n",
        "        if len(self.memory) < batch_size:\n",
        "            return None\n",
        "\n",
        "        # Sample batch from memory\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample(batch_size)\n",
        "\n",
        "        # Convert to tensors\n",
        "        states = torch.FloatTensor(states).to(self.device)\n",
        "        actions = torch.LongTensor(actions).to(self.device)\n",
        "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
        "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
        "        dones = torch.FloatTensor(dones).to(self.device)\n",
        "\n",
        "        # Current Q values: Q(s, a)\n",
        "        current_q_values = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
        "\n",
        "        # Next Q values from target network: max_a' Q_target(s', a')\n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.target_net(next_states).max(1)[0]\n",
        "            # Bellman equation: Q_target = r + gamma * max_a' Q(s', a') * (1 - done)\n",
        "            target_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n",
        "\n",
        "        # Compute loss (Mean Squared Error)\n",
        "        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n",
        "\n",
        "        # Optimize the network\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"Copy weights from policy network to target network\"\"\"\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"Decrease exploration rate\"\"\"\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def save(self, filepath):\n",
        "        \"\"\"Save model weights\"\"\"\n",
        "        torch.save({\n",
        "            'policy_net': self.policy_net.state_dict(),\n",
        "            'target_net': self.target_net.state_dict(),\n",
        "            'optimizer': self.optimizer.state_dict(),\n",
        "            'epsilon': self.epsilon\n",
        "        }, filepath)\n",
        "\n",
        "    def load(self, filepath):\n",
        "        \"\"\"Load model weights\"\"\"\n",
        "        checkpoint = torch.load(filepath)\n",
        "        self.policy_net.load_state_dict(checkpoint['policy_net'])\n",
        "        self.target_net.load_state_dict(checkpoint['target_net'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        self.epsilon = checkpoint['epsilon']\n",
        "\n",
        "# Test the agent\n",
        "print(\"Testing DQN Agent...\")\n",
        "test_agent = DQNAgent(state_shape=(4, 84, 84), action_size=18)\n",
        "test_state = np.random.rand(4, 84, 84)\n",
        "test_action = test_agent.select_action(test_state)\n",
        "print(f\"Agent selected action: {test_action}\")\n",
        "print(f\"Initial epsilon: {test_agent.epsilon}\")\n",
        "print(\"DQN Agent is ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQFP3Ink70tO",
        "outputId": "f6474b89-be4f-4715-a7d5-b69b8f6a1d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing DQN Agent...\n",
            "Agent using device: cuda\n",
            "Agent selected action: 16\n",
            "Initial epsilon: 1.0\n",
            "DQN Agent is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # Mount Google Drive and Setup Paths\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create directory in your Google Drive\n",
        "drive_base_path = '/content/drive/MyDrive/BankHeist_DQN_Project'\n",
        "os.makedirs(drive_base_path, exist_ok=True)\n",
        "\n",
        "print(f\"Google Drive mounted successfully!\")\n",
        "print(f\"Project directory: {drive_base_path}\")\n",
        "print(f\"All checkpoints, models, and metrics will be saved here\")\n",
        "print(\"-\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOHXxaI98Xer",
        "outputId": "bc34fb62-05e7-4ff0-dc8d-a33424380ccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully!\n",
            "Project directory: /content/drive/MyDrive/BankHeist_DQN_Project\n",
            "All checkpoints, models, and metrics will be saved here\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 9: Memory-Efficient Trainer with Local + Drive Backup\n",
        "import time\n",
        "import json\n",
        "import sys\n",
        "import gc\n",
        "import shutil\n",
        "\n",
        "class BankHeistTrainer:\n",
        "    \"\"\"Memory-efficient trainer with auto-recovery\"\"\"\n",
        "    def __init__(self, save_dir='./bank_heist_checkpoints', drive_backup_dir=None):\n",
        "        self.save_dir = save_dir\n",
        "        self.drive_backup_dir = drive_backup_dir\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        if drive_backup_dir:\n",
        "            os.makedirs(drive_backup_dir, exist_ok=True)\n",
        "\n",
        "        self.episode_rewards = []\n",
        "        self.episode_steps = []\n",
        "        self.episode_losses = []\n",
        "        self.epsilon_history = []\n",
        "\n",
        "    def train(self, agent=None, start_episode=0, total_episodes=500,\n",
        "              max_steps=2000, batch_size=32, learning_rate=0.00025,\n",
        "              gamma=0.99, epsilon_start=1.0, epsilon_min=0.01,\n",
        "              epsilon_decay=0.995, memory_size=30000,  # REDUCED from 50000\n",
        "              target_update_freq=10, print_freq=50,\n",
        "              local_save_freq=50, drive_save_freq=200):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            local_save_freq: Save to local disk every N episodes (fast)\n",
        "            drive_save_freq: Copy to Drive every N episodes (slow but safe)\n",
        "        \"\"\"\n",
        "\n",
        "        env = gym.make('ALE/BankHeist-v5', render_mode='rgb_array')\n",
        "        preprocessor = FramePreprocessor()\n",
        "\n",
        "        if agent is None:\n",
        "            agent = DQNAgent(\n",
        "                state_shape=(4, 84, 84), action_size=18,\n",
        "                learning_rate=learning_rate, gamma=gamma,\n",
        "                epsilon_start=epsilon_start, epsilon_min=epsilon_min,\n",
        "                epsilon_decay=epsilon_decay, memory_size=memory_size\n",
        "            )\n",
        "            print(f\"New agent created | Memory: {memory_size}\")\n",
        "        else:\n",
        "            print(f\"Resuming from episode {start_episode}\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        for episode in range(start_episode, start_episode + total_episodes):\n",
        "            state, _ = env.reset()\n",
        "            state = preprocessor.reset(state)\n",
        "            episode_reward = 0\n",
        "            episode_loss = []\n",
        "\n",
        "            for step in range(max_steps):\n",
        "                action = agent.select_action(state, training=True)\n",
        "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated\n",
        "                next_state = preprocessor.add_frame(next_state)\n",
        "                agent.memory.push(state, action, reward, next_state, done)\n",
        "\n",
        "                loss = agent.train_step(batch_size)\n",
        "                if loss is not None:\n",
        "                    episode_loss.append(loss)\n",
        "\n",
        "                episode_reward += reward\n",
        "                state = next_state\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            if episode % target_update_freq == 0:\n",
        "                agent.update_target_network()\n",
        "            agent.decay_epsilon()\n",
        "\n",
        "            self.episode_rewards.append(episode_reward)\n",
        "            self.episode_steps.append(step + 1)\n",
        "            self.episode_losses.append(np.mean(episode_loss) if episode_loss else 0)\n",
        "            self.epsilon_history.append(agent.epsilon)\n",
        "\n",
        "            # FAST LOCAL SAVE (every 50 episodes)\n",
        "            if (episode + 1) % local_save_freq == 0:\n",
        "                self._save_local_checkpoint(agent, episode + 1)\n",
        "\n",
        "            # SLOW DRIVE BACKUP (every 200 episodes)\n",
        "            if self.drive_backup_dir and (episode + 1) % drive_save_freq == 0:\n",
        "                self._backup_to_drive()\n",
        "                print(f\"  Backed up to Drive at episode {episode + 1}\")\n",
        "                sys.stdout.flush()\n",
        "\n",
        "            # Print progress\n",
        "            if (episode + 1) % print_freq == 0:\n",
        "                avg_reward = np.mean(self.episode_rewards[-min(100, len(self.episode_rewards)):])\n",
        "                elapsed = time.time() - start_time\n",
        "                eps_done = episode - start_episode + 1\n",
        "                remaining = (elapsed/eps_done)*(total_episodes - eps_done)/60\n",
        "                print(f\"Ep {episode+1} | R:{episode_reward:.0f} | \"\n",
        "                      f\"Avg:{avg_reward:.1f} | ε:{agent.epsilon:.3f} | \"\n",
        "                      f\"Mem:{len(agent.memory)} | {remaining:.0f}m\")\n",
        "                sys.stdout.flush()\n",
        "\n",
        "        env.close()\n",
        "\n",
        "        # Final save to both local and Drive\n",
        "        final_checkpoint = f\"final_ep{start_episode + total_episodes}.pt\"\n",
        "        final_metrics = f\"metrics_ep{start_episode + total_episodes}.json\"\n",
        "\n",
        "        agent.save(f\"{self.save_dir}/{final_checkpoint}\")\n",
        "        self.save_metrics(f\"{self.save_dir}/{final_metrics}\")\n",
        "\n",
        "        if self.drive_backup_dir:\n",
        "            self._backup_to_drive()\n",
        "            print(\"Final backup to Drive complete\")\n",
        "\n",
        "        elapsed = (time.time()-start_time)/60\n",
        "        final_avg = np.mean(self.episode_rewards[-min(100, len(self.episode_rewards)):])\n",
        "        print(f\"\\nComplete: {elapsed:.1f}m | Avg:{final_avg:.1f} | ε:{agent.epsilon:.3f}\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        return agent\n",
        "\n",
        "    def _save_local_checkpoint(self, agent, episode):\n",
        "        \"\"\"Quick save to local disk\"\"\"\n",
        "        agent.save(f\"{self.save_dir}/checkpoint_ep{episode}.pt\")\n",
        "        self.save_metrics(f\"{self.save_dir}/metrics_ep{episode}.json\")\n",
        "\n",
        "    def _backup_to_drive(self):\n",
        "        \"\"\"Copy all local files to Drive\"\"\"\n",
        "        if self.drive_backup_dir:\n",
        "            for filename in os.listdir(self.save_dir):\n",
        "                src = os.path.join(self.save_dir, filename)\n",
        "                dst = os.path.join(self.drive_backup_dir, filename)\n",
        "                shutil.copy2(src, dst)\n",
        "\n",
        "    def save_metrics(self, filepath):\n",
        "        metrics = {\n",
        "            'episode_rewards': self.episode_rewards,\n",
        "            'episode_steps': self.episode_steps,\n",
        "            'episode_losses': self.episode_losses,\n",
        "            'epsilon_history': self.epsilon_history,\n",
        "            'total_episodes': len(self.episode_rewards),\n",
        "            'avg_reward_last_100': float(np.mean(self.episode_rewards[-min(100, len(self.episode_rewards)):])),\n",
        "            'avg_steps_last_100': float(np.mean(self.episode_steps[-min(100, len(self.episode_steps)):]))\n",
        "        }\n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump(metrics, f, indent=2)\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Free memory after experiment\"\"\"\n",
        "        self.episode_rewards.clear()\n",
        "        self.episode_steps.clear()\n",
        "        self.episode_losses.clear()\n",
        "        self.epsilon_history.clear()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"Memory cleared\")\n",
        "\n",
        "print(\"Memory-efficient trainer ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wP_XbkkHBUTx",
        "outputId": "78b91cc0-ecd5-4359-cc6d-50a52d8f4abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory-efficient trainer ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 10: Load Checkpoint and Resume Training\n",
        "def load_checkpoint_and_resume(checkpoint_path, trainer):\n",
        "    \"\"\"Load a checkpoint and return agent + last episode number\"\"\"\n",
        "    agent = DQNAgent(\n",
        "        state_shape=(4, 84, 84), action_size=18,\n",
        "        learning_rate=0.00025, gamma=0.99,\n",
        "        epsilon_start=1.0, epsilon_min=0.01,\n",
        "        epsilon_decay=0.995, memory_size=30000\n",
        "    )\n",
        "    agent.load(checkpoint_path)\n",
        "\n",
        "    # Extract episode number from filename\n",
        "    import re\n",
        "    match = re.search(r'ep(\\d+)', checkpoint_path)\n",
        "    last_episode = int(match.group(1)) if match else 0\n",
        "\n",
        "    print(f\"Loaded checkpoint from episode {last_episode}\")\n",
        "    print(f\"Agent epsilon: {agent.epsilon}\")\n",
        "\n",
        "    return agent, last_episode\n",
        "\n",
        "# Example usage if you need to resume:\n",
        "# agent, last_ep = load_checkpoint_and_resume('./local_baseline/checkpoint_ep400.pt', trainer_baseline)\n",
        "# Then continue training from last_ep\n",
        "\n",
        "print(\"Recovery helper ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d32GzNM4BaqF",
        "outputId": "e7591e1b-c811-42b2-d9cf-2f6656d16c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recovery helper ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 11: Baseline Batch 1 (Episodes 1-400)\n",
        "local_dir = './local_baseline'\n",
        "drive_dir = f\"{drive_base_path}/baseline_run\"\n",
        "\n",
        "trainer_baseline = BankHeistTrainer(\n",
        "    save_dir=local_dir,\n",
        "    drive_backup_dir=drive_dir\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"BASELINE - BATCH 1: Episodes 1-400\")\n",
        "print(\"Saving locally every 50 eps, Drive backup every 200 eps\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "agent_baseline = trainer_baseline.train(\n",
        "    agent=None,\n",
        "    start_episode=0,\n",
        "    total_episodes=400,\n",
        "    max_steps=2000,\n",
        "    batch_size=32,\n",
        "    learning_rate=0.00025,\n",
        "    gamma=0.99,\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_min=0.01,\n",
        "    epsilon_decay=0.995,\n",
        "    memory_size=30000,\n",
        "    target_update_freq=10,\n",
        "    print_freq=50,\n",
        "    local_save_freq=50,\n",
        "    drive_save_freq=200\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArrRzkNGBc6u",
        "outputId": "751a0d41-96b9-45d9-9805-0b5ea0a88040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "BASELINE - BATCH 1: Episodes 1-400\n",
            "Saving locally every 50 eps, Drive backup every 200 eps\n",
            "============================================================\n",
            "Agent using device: cuda\n",
            "New agent created | Memory: 30000\n",
            "Ep 50 | R:10 | Avg:15.2 | ε:0.778 | Mem:26609 | 31m\n",
            "Ep 100 | R:80 | Avg:14.9 | ε:0.606 | Mem:30000 | 26m\n",
            "Ep 150 | R:20 | Avg:17.1 | ε:0.471 | Mem:30000 | 22m\n",
            "  Backed up to Drive at episode 200\n",
            "Ep 200 | R:10 | Avg:21.8 | ε:0.367 | Mem:30000 | 18m\n",
            "Ep 250 | R:20 | Avg:23.7 | ε:0.286 | Mem:30000 | 14m\n",
            "Ep 300 | R:20 | Avg:26.8 | ε:0.222 | Mem:30000 | 9m\n",
            "Ep 350 | R:20 | Avg:29.3 | ε:0.173 | Mem:30000 | 5m\n",
            "  Backed up to Drive at episode 400\n",
            "Ep 400 | R:10 | Avg:27.9 | ε:0.135 | Mem:30000 | 0m\n",
            "Final backup to Drive complete\n",
            "\n",
            "Complete: 37.2m | Avg:27.9 | ε:0.135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick test - see how long episodes actually last\n",
        "env = gym.make('ALE/BankHeist-v5', render_mode='rgb_array')\n",
        "preprocessor = FramePreprocessor()\n",
        "\n",
        "print(\"Testing 10 episodes to see natural length...\")\n",
        "step_counts = []\n",
        "\n",
        "for i in range(10):\n",
        "    state, _ = env.reset()\n",
        "    state = preprocessor.reset(state)\n",
        "\n",
        "    for step in range(5000):  # High limit\n",
        "        action = env.action_space.sample()  # Random actions\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "        if terminated or truncated:\n",
        "            step_counts.append(step + 1)\n",
        "            print(f\"Episode {i+1}: {step+1} steps\")\n",
        "            break\n",
        "\n",
        "env.close()\n",
        "\n",
        "print(f\"\\nAverage episode length: {np.mean(step_counts):.0f} steps\")\n",
        "print(f\"Max episode length: {max(step_counts)} steps\")\n",
        "print(f\"Min episode length: {min(step_counts)} steps\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjOsgh1hJd9Q",
        "outputId": "b174e6c3-e03e-4ec7-ccd8-a64f134d0d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing 10 episodes to see natural length...\n",
            "Episode 1: 838 steps\n",
            "Episode 2: 668 steps\n",
            "Episode 3: 388 steps\n",
            "Episode 4: 584 steps\n",
            "Episode 5: 495 steps\n",
            "Episode 6: 808 steps\n",
            "Episode 7: 699 steps\n",
            "Episode 8: 443 steps\n",
            "Episode 9: 656 steps\n",
            "Episode 10: 364 steps\n",
            "\n",
            "Average episode length: 594 steps\n",
            "Max episode length: 838 steps\n",
            "Min episode length: 364 steps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 12: Baseline Batch 2 (Episodes 401-800)\n",
        "print(\"=\" * 60)\n",
        "print(\"BASELINE - BATCH 2: Episodes 401-800\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "agent_baseline = trainer_baseline.train(\n",
        "    agent=agent_baseline,\n",
        "    start_episode=400,\n",
        "    total_episodes=400,\n",
        "    max_steps=2000,\n",
        "    batch_size=32,\n",
        "    print_freq=50,\n",
        "    local_save_freq=50,\n",
        "    drive_save_freq=200\n",
        ")\n",
        "\n",
        "# Final save and cleanup\n",
        "agent_baseline.save(f\"{drive_dir}/final_baseline.pt\")\n",
        "trainer_baseline.save_metrics(f\"{drive_dir}/final_baseline_metrics.json\")\n",
        "print(\"Baseline complete!\")\n",
        "\n",
        "# Free memory before next experiment\n",
        "del agent_baseline\n",
        "trainer_baseline.cleanup()\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"Memory freed for next experiment\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsTnIkugBe1I",
        "outputId": "e7e04aee-5f30-47d0-b8a3-7fcfbb7fc26a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "BASELINE - BATCH 2: Episodes 401-800\n",
            "============================================================\n",
            "Resuming from episode 400\n",
            "Ep 450 | R:40 | Avg:33.5 | ε:0.096 | Mem:30000 | 36m\n",
            "Ep 500 | R:10 | Avg:37.7 | ε:0.075 | Mem:30000 | 33m\n",
            "Ep 550 | R:50 | Avg:41.9 | ε:0.058 | Mem:30000 | 27m\n",
            "  Backed up to Drive at episode 600\n",
            "Ep 600 | R:80 | Avg:62.9 | ε:0.045 | Mem:30000 | 22m\n",
            "Ep 650 | R:110 | Avg:89.8 | ε:0.035 | Mem:30000 | 17m\n",
            "Ep 700 | R:200 | Avg:137.1 | ε:0.027 | Mem:30000 | 12m\n",
            "Ep 750 | R:190 | Avg:195.2 | ε:0.021 | Mem:30000 | 6m\n",
            "  Backed up to Drive at episode 800\n",
            "Ep 800 | R:90 | Avg:215.7 | ε:0.017 | Mem:30000 | 0m\n",
            "Final backup to Drive complete\n",
            "\n",
            "Complete: 52.6m | Avg:215.7 | ε:0.017\n",
            "Baseline complete!\n",
            "Memory cleared\n",
            "Memory freed for next experiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reconnect after crash\n",
        "from google.colab import drive\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "drive_base_path = '/content/drive/MyDrive/BankHeist_DQN_Project'\n",
        "\n",
        "# Verify baseline is saved\n",
        "print(\"Checking saved baseline...\")\n",
        "with open(f\"{drive_base_path}/baseline_run/final_baseline_metrics.json\", 'r') as f:\n",
        "    baseline_metrics = json.load(f)\n",
        "\n",
        "print(f\"Baseline (800 eps): {baseline_metrics['avg_reward_last_100']:.1f} points\")\n",
        "print(\"Ready to start experiments!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOalyLNIzB9a",
        "outputId": "4243ca63-3a84-4937-bbfa-271e37aa8dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Checking saved baseline...\n",
            "Baseline (800 eps): 215.7 points\n",
            "Ready to start experiments!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL: Experiment 3 - Slower Epsilon Decay (400 episodes)\n",
        "import gc\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "local_dir_exp3 = './local_exp3'\n",
        "drive_dir_exp3 = f\"{drive_base_path}/experiment3_epsilon\"\n",
        "\n",
        "trainer_exp3 = BankHeistTrainer(\n",
        "    save_dir=local_dir_exp3,\n",
        "    drive_backup_dir=drive_dir_exp3\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EXPERIMENT 3: Slower Epsilon Decay (0.99 vs baseline 0.995)\")\n",
        "print(\"Testing if more exploration improves learning\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "agent_exp3 = trainer_exp3.train(\n",
        "    agent=None,\n",
        "    start_episode=0,\n",
        "    total_episodes=400,\n",
        "    max_steps=2000,\n",
        "    batch_size=32,\n",
        "    learning_rate=0.00025,\n",
        "    gamma=0.99,\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_min=0.01,\n",
        "    epsilon_decay=0.99,  # CHANGED: Slower decay (baseline is 0.995)\n",
        "    memory_size=30000,\n",
        "    target_update_freq=10,\n",
        "    print_freq=50,\n",
        "    local_save_freq=50,\n",
        "    drive_save_freq=200\n",
        ")\n",
        "\n",
        "agent_exp3.save(f\"{drive_dir_exp3}/final_exp3.pt\")\n",
        "trainer_exp3.save_metrics(f\"{drive_dir_exp3}/final_exp3_metrics.json\")\n",
        "\n",
        "del agent_exp3\n",
        "trainer_exp3.cleanup()\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"Experiment 3 complete, memory cleared\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5m1C5UWKrwT",
        "outputId": "ecc33b01-92aa-446b-e7b0-8cd4cc5b1fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "EXPERIMENT 3: Slower Epsilon Decay (0.99 vs baseline 0.995)\n",
            "Testing if more exploration improves learning\n",
            "============================================================\n",
            "Agent using device: cuda\n",
            "New agent created | Memory: 30000\n",
            "Ep 50 | R:10 | Avg:12.6 | ε:0.605 | Mem:25339 | 29m\n",
            "Ep 100 | R:10 | Avg:15.1 | ε:0.366 | Mem:30000 | 27m\n",
            "Ep 150 | R:40 | Avg:20.9 | ε:0.221 | Mem:30000 | 23m\n",
            "  Backed up to Drive at episode 200\n",
            "Ep 200 | R:30 | Avg:26.9 | ε:0.134 | Mem:30000 | 19m\n",
            "Ep 250 | R:20 | Avg:30.2 | ε:0.081 | Mem:30000 | 15m\n",
            "Ep 300 | R:30 | Avg:31.0 | ε:0.049 | Mem:30000 | 10m\n",
            "Ep 350 | R:110 | Avg:39.7 | ε:0.030 | Mem:30000 | 5m\n",
            "  Backed up to Drive at episode 400\n",
            "Ep 400 | R:170 | Avg:71.9 | ε:0.018 | Mem:30000 | 0m\n",
            "Final backup to Drive complete\n",
            "\n",
            "Complete: 45.4m | Avg:71.9 | ε:0.018\n",
            "Memory cleared\n",
            "Experiment 3 complete, memory cleared\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL: Final Results Summary\n",
        "import json\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"FINAL RESULTS - ALL EXPERIMENTS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# All your results\n",
        "results = {\n",
        "    'Baseline (1000 eps)': 493.0,\n",
        "    'Baseline (400 eps)': 27.9,\n",
        "    'Exp 1: Higher Alpha (α=0.0005)': 8.8,\n",
        "    'Exp 2: Lower Gamma (γ=0.95)': 23.0,\n",
        "    'Exp 3: Slower Epsilon (decay=0.99)': 71.9,\n",
        "    'Exp 4: Boltzmann Policy': 38.7\n",
        "}\n",
        "\n",
        "print(\"\\n1. PERFORMANCE AT 400 EPISODES\")\n",
        "print(\"-\" * 70)\n",
        "baseline_400 = 27.9\n",
        "for name, score in [\n",
        "    ('Baseline', 27.9),\n",
        "    ('Exp 1: Higher Alpha', 8.8),\n",
        "    ('Exp 2: Lower Gamma', 23.0),\n",
        "    ('Exp 3: Slower Epsilon', 71.9),\n",
        "    ('Exp 4: Boltzmann', 38.7)\n",
        "]:\n",
        "    change = ((score/baseline_400)-1)*100\n",
        "    marker = \" ← WINNER\" if score == 71.9 else \"\"\n",
        "    print(f\"{name:30s}: {score:5.1f} ({change:+6.1f}%){marker}\")\n",
        "\n",
        "print(\"\\n2. KEY FINDINGS\")\n",
        "print(\"-\" * 70)\n",
        "print(\"Winner: Experiment 3 (Slower Epsilon Decay)\")\n",
        "print(\"  • 71.9 avg at 400 episodes (158% improvement)\")\n",
        "print(\"  • Slower decay (0.99 vs 0.995) = more exploration\")\n",
        "print(\"  • Better learning rate than all other configurations\")\n",
        "print()\n",
        "print(\"Best Overall: Baseline at 1000 episodes (493.0 avg)\")\n",
        "print(\"  • Shows continued improvement with more training\")\n",
        "print(\"  • Exp 3 would likely exceed this with 1000 episodes\")\n",
        "\n",
        "print(\"\\n3. TOTAL TRAINING\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"Total episodes: 2700\")\n",
        "print(f\"  • Baseline: 1000 episodes\")\n",
        "print(f\"  • Experiments 1-4: 1700 episodes\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Save summary\n",
        "summary = {\n",
        "    'all_results': results,\n",
        "    'winner': 'Experiment 3: Slower Epsilon Decay',\n",
        "    'winner_score': 71.9,\n",
        "    'improvement': 158,\n",
        "    'total_episodes': 2700\n",
        "}\n",
        "\n",
        "with open(f'{drive_base_path}/final_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"\\nSummary saved: {drive_base_path}/final_summary.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwSLFeIgIhkR",
        "outputId": "c2907ef8-6316-40e8-b8be-8e2dc43e76b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "FINAL RESULTS - ALL EXPERIMENTS\n",
            "======================================================================\n",
            "\n",
            "1. PERFORMANCE AT 400 EPISODES\n",
            "----------------------------------------------------------------------\n",
            "Baseline                      :  27.9 (  +0.0%)\n",
            "Exp 1: Higher Alpha           :   8.8 ( -68.5%)\n",
            "Exp 2: Lower Gamma            :  23.0 ( -17.6%)\n",
            "Exp 3: Slower Epsilon         :  71.9 (+157.7%) ← WINNER\n",
            "Exp 4: Boltzmann              :  38.7 ( +38.7%)\n",
            "\n",
            "2. KEY FINDINGS\n",
            "----------------------------------------------------------------------\n",
            "Winner: Experiment 3 (Slower Epsilon Decay)\n",
            "  • 71.9 avg at 400 episodes (158% improvement)\n",
            "  • Slower decay (0.99 vs 0.995) = more exploration\n",
            "  • Better learning rate than all other configurations\n",
            "\n",
            "Best Overall: Baseline at 1000 episodes (493.0 avg)\n",
            "  • Shows continued improvement with more training\n",
            "  • Exp 3 would likely exceed this with 1000 episodes\n",
            "\n",
            "3. TOTAL TRAINING\n",
            "----------------------------------------------------------------------\n",
            "Total episodes: 2700\n",
            "  • Baseline: 1000 episodes\n",
            "  • Experiments 1-4: 1700 episodes\n",
            "======================================================================\n",
            "\n",
            "Summary saved: /content/drive/MyDrive/BankHeist_DQN_Project/final_summary.json\n"
          ]
        }
      ]
    }
  ]
}